{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monotonic Classifiers\n",
    "\n",
    "Some classifiers should never \"flip-flop\" between classes. For example, consider the following classifier that labels system call traces from programs as being benign or malicious programs. No matter how many benign instructions are added to a malicious program, it should never trick the classifier into thinking it is benign.\n",
    "\n",
    "The classifier below takes sequences of system calls obtained from execution traces from malicious and benign programs. Treating each execution trace as a document, we extract a tf-idf[1] vector for feature extraction. Code below is provided that:\n",
    "1. Grabs ground truth traces\n",
    "1. Vectorizes them with tf-idf\n",
    "1. Performs 10-fold cross validation\n",
    "1. Trains a Logistic Regression model\n",
    "\n",
    "Your task as a malware author yourself, is to find direct and indirect ways to break this model so antivirus software cannot detect your code. Approach these tasks in three chunks:\n",
    "1. Manually manipulate a malicious feature vector such that the classifier mistakenly labels it malicious. If you successfully do this once, create a function that given a benign feature vector returns a new one that will be classified as benign.\n",
    "1. Identify features that, given your knowledge of what monotonic classifiers try to solve, could be used to \"flip\" a malicious program into benign one.\n",
    "1. Using the aforementioned features, write a function that transforms a malicious syscall trace (appending is fine) to be classified as benign.\n",
    "1. Modify the classifier so these features can no longer be used to \"flip\" a malicious trace. There's a quick'n'dirty way to do this, but more sophisticated[2] and robust[3] techniques exist if monotonicity is an important feature that your classifier needs.\n",
    "\n",
    "## References\n",
    "* [1] https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "* [2] https://arxiv.org/pdf/1804.03643.pdf\n",
    "* [3] https://www.slideshare.net/MSbluehat/bluehat-v17-detecting-compromise-on-windows-endpoints-with-osquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## utils\n",
    "def rwalk(directory, pattern):\n",
    "    \"\"\"Recursively search \"directory\" for files that match the Unix shell-style\n",
    "    wildcard given by \"pattern\" (like '*.mp3'). Returns matches as a generator.\"\"\"\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            yield os.path.join(root, filename)\n",
    "\n",
    "def gettraces(benignpath='../data/01-monotonic-classifiers/benign-traces',\n",
    "              malpath='../data/01-monotonic-classifiers/malicious-traces'):\n",
    "    return list(rwalk(malpath, '*.trace')), list(rwalk(benignpath, '*.trace'))\n",
    "\n",
    "def get_random_malicious_trace(malpath='../data/01-monotonic-classifiers/malicious-traces'):\n",
    "    \"\"\"Grab the text of a random malicious system call trace.\"\"\"\n",
    "    mal, _ = gettraces(malpath=malpath)\n",
    "    with open(random.choice(mal)) as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the TfidfVectorizer to vectorize ground truth\n",
    "\n",
    "The following extracts vectors from each benign and malicious execution trace and returns four values:\n",
    "1. `X`: the feature vectors\n",
    "1. `y`: the class labels\n",
    "1. `terms`: the list of labels (0 is benign, 1 is malicious)\n",
    "1. `vectorizer`: a TfidfVectorizer which is fit to the terms in the ground truth and can be used to fit new syscall traces with `vectorizer.transform([trace1, trace2, ..., traceN])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(featuredir='../data/01-monotonic-classifiers/feature-vectors'):\n",
    "    pos_traces, neg_traces = gettraces()\n",
    "    pos_y = [1 for _ in pos_traces]\n",
    "    neg_y = [0 for _ in neg_traces]\n",
    "    docs = [open(x).read() for x in pos_traces + neg_traces]\n",
    "    y = np.array(pos_y + neg_y)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    terms = np.asarray(vectorizer.get_feature_names())\n",
    "    return X, y, terms, vectorizer\n",
    "\n",
    "X, y, terms, vectorizer = vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _heatmap(crosstab):\n",
    "    plt.clf()\n",
    "    p = seaborn.heatmap(crosstab, square=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def _cv(X, y, k, name, clf, csvname, modeldir=None, terms=None, resultdir=None):\n",
    "    print('## %s' % name)\n",
    "    print('### Cross Validation')\n",
    "    print('`%s`' % str(cross_val_score(clf, X, y, cv=k)))\n",
    "    print('### CV Confusion Matrix')\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=k)\n",
    "    print('```')\n",
    "    print(pd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "    print('```')\n",
    "    _heatmap(pd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted'],\n",
    "                         normalize='index'))\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = _cv(X, y, 10, 'name', LogisticRegression(solver='lbfgs'), 'foo.csv', modeldir='../work', terms=terms, resultdir='../work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Attack\n",
    "\n",
    "You (somehow) have direct access to the feature vectors. Don't ask how, celebrate! Traces are stored in `../data/01-monotonic-classifiers/{malicious,benign}-traces/`. I recommend writing a functions that returns a feature vector given the path of a trace, comparing benign and malicious vectors, and manually transforming a malicious vector to a benign vector. If you have time, write a function to perform this manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indirect Attack Part 1 (Find terms)\n",
    "\n",
    "So we've made our malware undetectable (above), but we did so by directly manipulating the feature vector. This assumes a powerful and/or dedicated attacker and we're a bit lazy. How can we alter our malware's _behavior_ such that our malware is classified as benign software? Let's identify `terms` that are more likely to be associated with benign software than malicious software by finding _negative_ coefficients in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indirect Attack Part 2 (Adversarial Sample Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Classifier Monotonic\n",
    "\n",
    "A simple way to make a classifier monotonic for our purposes is to not allow attackers to abuse negative coefficient features. Add a vector to `clf.coef` such that there are no longer negative coefficients, and demonstrates this defeats your adversarial generation function from the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can accomplish this by:\n",
    "* Identify the index in `clf.coef_` for the feature you abused\n",
    "* Set its weight to `0.0`\n",
    "* Rerunning our classification examples from above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
