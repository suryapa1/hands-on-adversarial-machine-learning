{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGA Classifier\n",
    "\n",
    "The following code approximates the classification phase of Pleiades[1], a _domain-name generation algorithm_ (DGA) detection system. A DGA is used---typically by malware---to make a botnet's infrastructure agile, cheap, and resistant to takedown. At a high-level, a pseudorandom algorithm is seeded with something known to the botnet operator (e.g., current date or hash of the most recent Twitter post of a particular account) and generates `N` domains. These domains are used to connect to infrastructure, which tells the infected bots what to do. A botmaster can register only a few of these domains (saving money), while defenders must prevent _all_ from being registered to destroy a botnet. Luckily, these DGAs can be modeled!\n",
    "\n",
    "The following code classifies sets of domain names to the family of pseudorandom algorithms that generate them. A subset of Johannes Bacher's wonderful GitHub repo[2] was used for ground truth. The classification algorithm works roughly as follows:\n",
    "\n",
    "* Generate feature vector from set of domains. The features attempt to quantify the character distribution\n",
    "  * Within family, chunk domains into groups of `alpha = 10`\n",
    "  * For each 1,2,3,4-gram (character). Get mean, median, std. dev. of n-gram frequency (4 x 3 = 12 features)\n",
    "  * Mean & median of Shannon entropy[3] of registerable and subdomain (e.g., `google.com` and `mail.google.com`).\n",
    "  * Mean, med, std. dev, var of \n",
    "    * Length\n",
    "    * \\# of domain levels\n",
    "    * \\# distinct chars\n",
    "    * \\# distinct TLDs (e.g., `.com`)\n",
    "    * \\# of `.com` \n",
    "    * \\# of non-`.com`\n",
    "    * Ratio of \\# of `.com`/\\# of `.other`\n",
    "    * Mean/med/stddev TLD frequency\n",
    "\n",
    "Imagine you're using a botnet to spy on your competitors. However, they're on to you and have a model for your family of DGA. Try to break their model by making your DGA's domains look more like the DGA of some common, off-the-shelf malware so as not to arouse suspicion. Your goal is to:\n",
    "1. Defeat the DGA classifier by masquerading as another, and\n",
    "1. Use the above information to retrain the model to make it more robust (so they can't do the same to you!)\n",
    "\n",
    "For #1, try to compare two families by the properties above and generate a function to make the first class of domains \"look\" more like the second class of domains. You'll know you successfully generated an _adversarial example_ when the classifier incorrectly labels it. For #2, use your function to generate additional adversarial examples, use them to train a new model, and demonstrate that the new model is more robust against your function.\n",
    "\n",
    "## Notes\n",
    "* The `_build_ground_truth_dataframe` will generate some warnings. This is expected behavior. Carry on!\n",
    "\n",
    "## References\n",
    "\n",
    "* [1] https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final127.pdf\n",
    "* [2] https://github.com/baderj/domain_generation_algorithms\n",
    "* [3] https://en.wikipedia.org/wiki/Entropy_(information_theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import string\n",
    "import fnmatch\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import tldextract\n",
    "import numpy as np\n",
    "from scipy.stats import describe\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prevent tldextract fetching\n",
    "parsedomain = tldextract.TLDExtract(suffix_list_urls=None)\n",
    "\n",
    "## Utils\n",
    "def rwalk(directory, pattern):\n",
    "    \"\"\"Recursively search \"directory\" for files that match the Unix shell-style\n",
    "    wildcard given by \"pattern\" (like '*.mp3'). Returns matches as a generator.\"\"\"\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            yield os.path.join(root, filename)\n",
    "\n",
    "def chunks(l, n, slide=None):\n",
    "    \"\"\"Yield successive `n`-sized chunks from iterable `l` with a sliding window of `slide`\n",
    "    indexes. Default value of slide has non-overlapping chunks.\"\"\"\n",
    "    if slide is None: slide = n\n",
    "    for i in range(0, len(l), slide):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "def H(s):\n",
    "    \"\"\"Compute the Shannon entropy of an iterable\"\"\"\n",
    "    p, lns = Counter(s), float(len(s))\n",
    "    return -sum(count/lns * math.log(count/lns, 2) for count in p.values())\n",
    "\n",
    "## Classification helpers\n",
    "def _load_ground_truth(dgadir='../data/dga/ground-truth', class_subset=set(['banjori', 'kraken', 'qakbot', 'shiotob', 'simda'])):\n",
    "    \"\"\"Retrieve a map of class labels to a list of domains to train on. Params:\n",
    "    * `dgadir`: Default directory to grab ground truth from\n",
    "    * `class_subset`: Subset of class labels to consider.\n",
    "    \n",
    "    Hint! Use this to identify what the domain names \"look like\" for different classes.\"\"\"\n",
    "    def fixup(d):\n",
    "        return d.lower().strip()\n",
    "    classtodomains = {}\n",
    "    for path in rwalk(dgadir, '*.txt'):\n",
    "        with open(path) as f:\n",
    "            class_ = os.path.basename(os.path.dirname(path))\n",
    "            if class_subset is not None and class_ not in class_subset:\n",
    "                continue\n",
    "            classtodomains[class_] = list(map(fixup, f.readlines()))\n",
    "\n",
    "    return classtodomains\n",
    "\n",
    "def _ngrams(domains, ns=[1, 2, 3, 4]):\n",
    "    def stats(counter, n):\n",
    "        # you can use the below to see this w.r.t. all possibilities.\n",
    "        #possibilities = len(string.ascii_lowercase + string.digits + '-.')**n\n",
    "        l = list(counter.values())\n",
    "        res = describe(l)\n",
    "        median = np.median(l)\n",
    "\n",
    "        return [res.mean, median, math.sqrt(res.variance)]\n",
    "\n",
    "    features = []\n",
    "    for n in ns:\n",
    "        for domain in domains:\n",
    "            domain = domain.lower()\n",
    "            c = Counter([chunk for chunk in chunks(domain, n, slide=1) if len(chunk) == n])\n",
    "            features.extend(stats(c, n))\n",
    "\n",
    "    return features\n",
    "\n",
    "def _e2and3ld_entropy(domain):\n",
    "    def onlylast(domain):\n",
    "        return domain.split('.')[-1]\n",
    "    domain = domain.lower()\n",
    "    res = parsedomain(domain)\n",
    "    e2ld = '%s.%s' % (res.domain, res.suffix)\n",
    "    if res.subdomain:\n",
    "        e3ld = '%s.%s.%s' % (onlylast(res.subdomain),\n",
    "                             res.domain,\n",
    "                             res.suffix)\n",
    "    else:\n",
    "        e3ld = e2ld\n",
    "\n",
    "    return H(e2ld), H(e3ld)\n",
    "\n",
    "def _entropy(domains):\n",
    "    perdomain = {domain: _e2and3ld_entropy(domain) for domain in domains}\n",
    "    e2ld_h = [x[0] for x in perdomain.values()]\n",
    "    e3ld_h = [x[1] for x in perdomain.values()]\n",
    "\n",
    "    return (perdomain, [describe(e2ld_h).mean, np.median(e2ld_h),\n",
    "                        describe(e3ld_h).mean, np.median(e3ld_h)])\n",
    "\n",
    "def _len(domains):\n",
    "    lengths = list(map(len, domains))\n",
    "    res = describe(lengths)\n",
    "    return [res.mean, np.median(lengths), math.sqrt(res.variance), res.variance]\n",
    "\n",
    "def _levels(domains):\n",
    "    def numlevels(d):\n",
    "        return len(list(filter(lambda x: x == '.', d))) + 1\n",
    "    numlds = [numlevels(d) for d in domains]\n",
    "    res = describe(numlds)\n",
    "    return [res.mean, np.median(numlds), math.sqrt(res.variance), res.variance]\n",
    "\n",
    "def _tlds(domains):\n",
    "    return len(Counter([x.split('.')[-1] for x in domains]))\n",
    "\n",
    "def _distinctchars(domains):\n",
    "    return len(Counter(''.join(domains)))\n",
    "\n",
    "def vectorize(domains):\n",
    "    features = []\n",
    "    features.extend(_ngrams(domains))\n",
    "    features.extend(_entropy(domains)[1])\n",
    "    features.extend(_len(domains))\n",
    "    features.extend(_levels(domains))\n",
    "    features.append(_tlds(domains))\n",
    "    features.append(_distinctchars(domains))\n",
    "    return features\n",
    "\n",
    "def _build_ground_truth_dataframe(alpha=10, dgadir='../data/dga/ground-truth', class_subset=set(['banjori', 'kraken', 'qakbot', 'shiotob', 'simda'])):\n",
    "    X, y = [], []\n",
    "    for class_, domains in tqdm_notebook(_load_ground_truth(dgadir=dgadir, class_subset=class_subset).items(), desc='Classes', position=0):\n",
    "        for domain_chunks in chunks(domains, alpha):\n",
    "            features = vectorize(domain_chunks)\n",
    "            if len(features) != 134:\n",
    "                #sys.stderr.write('Error with chunk from %s\\n' % class_)\n",
    "                continue\n",
    "            X.append(features)\n",
    "            y.append(class_)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def _heatmap(crosstab):\n",
    "    plt.clf()\n",
    "    p = seaborn.heatmap(crosstab, square=True, vmin=0.0, vmax=1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def _cv(X, y, k, name, clf, csvname, modeldir=None, terms=None, resultdir=None):\n",
    "    print('## %s' % name)\n",
    "    print('### Cross Validation')\n",
    "    print('`%s`' % str(cross_val_score(clf, X, y, cv=k)))\n",
    "    print('### CV Confusion Matrix')\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=k)\n",
    "    print('```')\n",
    "    print(pd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "    print('```')\n",
    "\n",
    "    _heatmap(pd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted'],\n",
    "                         normalize='index'))\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def save(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ground truth dataframe, cross validate, and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = _build_ground_truth_dataframe()\n",
    "clf = _cv(X, y, 10, 'DGA Classifier', RandomForestClassifier(n_estimators=100), 'dga-classifier.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break DGA Classifier\n",
    "Let's try to break the classifier. The rough idea is as follows:\n",
    "* Examine the domains from the five classes\n",
    "* Identify similarities and differences between them\n",
    "* Choose an initial and target class. Now your goal is to make domains from the initial class \"look like\" the target class. I would recommend looking at:\n",
    "  * Which characters are used\n",
    "  * Which TLD (e.g., `.com` and `.net`) are used\n",
    "  * Length (is it fixed? is it random within some range?)\n",
    "* There's no one way to do this!\n",
    "\n",
    "Some helpful hints:\n",
    "* The classifier works on a group of `alpha=10` domains, i.e., 10 domains -> 1 feature vector.\n",
    "* Take a look at the `_load_ground_truth()` function to retrieve and examine domains.\n",
    "* `describe()` is your friend!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix DGA Classifier\n",
    "Using your function from the previous step, generate a set of _adversarial examples_ and use those to retrain the classifier. Show that it's more resistant. I would recommend:\n",
    "* Generating the original ground truth dataset\n",
    "* Appending your new vectors/labels to X and y\n",
    "* Rerunning `_cv` to get a new classifier\n",
    "* Checking if the new classifier can handle your perturbations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
