{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tldextract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-77be0d5ad69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtldextract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tldextract'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import string\n",
    "import fnmatch\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import tldextract\n",
    "import numpy as np\n",
    "from scipy.stats import describe\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Utils\n",
    "def rwalk(directory, pattern):\n",
    "    \"\"\"Recursively search \"directory\" for files that match the Unix shell-style\n",
    "        wildcard given by \"pattern\" (like '*.mp3'). Returns matches as a generator.\"\"\"\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            yield os.path.join(root, filename)\n",
    "\n",
    "def chunks(l, n, slide=None):\n",
    "    \"\"\"Yield successive n-sized chunks from l with a sliding window of slide\n",
    "    indexes. Default value of slide has non-overlapping chunks.\"\"\"\n",
    "    if slide is None: slide = n\n",
    "    for i in range(0, len(l), slide):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "def H(s):\n",
    "    p, lns = Counter(s), float(len(s))\n",
    "    return -sum(count/lns * math.log(count/lns, 2) for count in p.values())\n",
    "\n",
    "## Classification stuff\n",
    "def _load_ground_truth(dgadir='../data/dga/ground-truth'):\n",
    "    def fixup(d):\n",
    "        return d.lower().strip()\n",
    "    classtodomains = {}\n",
    "    for path in rwalk(dgadir, '*.txt'):\n",
    "        with open(path) as f:\n",
    "            class_ = os.path.basename(os.path.dirname(path))\n",
    "            classtodomains[class_] = list(map(fixup, f.readlines()))\n",
    "\n",
    "    return classtodomains\n",
    "\n",
    "def _ngrams(domains, ns=[1, 2, 3, 4]):\n",
    "    def stats(counter, n):\n",
    "        # you can use the below to see this w.r.t. all possibilities.\n",
    "        #possibilities = len(string.ascii_lowercase + string.digits + '-.')**n\n",
    "        l = list(counter.values())\n",
    "        res = describe(l)\n",
    "        median = np.median(l)\n",
    "\n",
    "        return [res.mean, median, math.sqrt(res.variance)]\n",
    "\n",
    "    features = []\n",
    "    for n in ns:\n",
    "        for domain in domains:\n",
    "            domain = domain.lower()\n",
    "            c = Counter([chunk for chunk in chunks(domain, n, slide=1) if len(chunk) == n])\n",
    "            features.extend(stats(c, n))\n",
    "\n",
    "    return features\n",
    "\n",
    "def _e2and3ld_entropy(domain):\n",
    "    def onlylast(domain):\n",
    "        return domain.split('.')[-1]\n",
    "    domain = domain.lower()\n",
    "    res = tldextract.extract(domain)\n",
    "    e2ld = '%s.%s' % (res.domain, res.suffix)\n",
    "    if res.subdomain:\n",
    "        e3ld = '%s.%s.%s' % (onlylast(res.subdomain),\n",
    "                             res.domain,\n",
    "                             res.suffix)\n",
    "    else:\n",
    "        e3ld = e2ld\n",
    "\n",
    "    return H(e2ld), H(e3ld)\n",
    "\n",
    "def _entropy(domains):\n",
    "    perdomain = {domain: _e2and3ld_entropy(domain) for domain in domains}\n",
    "    e2ld_h = [x[0] for x in perdomain.values()]\n",
    "    e3ld_h = [x[1] for x in perdomain.values()]\n",
    "\n",
    "    return (perdomain, [describe(e2ld_h).mean, np.median(e2ld_h),\n",
    "                        describe(e3ld_h).mean, np.median(e3ld_h)])\n",
    "\n",
    "def _len(domains):\n",
    "    lengths = list(map(len, domains))\n",
    "    res = describe(lengths)\n",
    "    return [res.mean, np.median(lengths), math.sqrt(res.variance), res.variance]\n",
    "\n",
    "def _levels(domains):\n",
    "    def numlevels(d):\n",
    "        return len(list(filter(lambda x: x == '.', d))) + 1\n",
    "    numlds = [numlevels(d) for d in domains]\n",
    "    res = describe(numlds)\n",
    "    return [res.mean, np.median(numlds), math.sqrt(res.variance), res.variance]\n",
    "\n",
    "def _tlds(domains):\n",
    "    return len(Counter([x.split('.')[-1] for x in domains]))\n",
    "\n",
    "def _distinctchars(domains):\n",
    "    return len(Counter(''.join(domains)))\n",
    "\n",
    "def vectorize(domains):\n",
    "    features = []\n",
    "    features.extend(_ngrams(domains))\n",
    "    features.extend(_entropy(domains)[1])\n",
    "    features.extend(_len(domains))\n",
    "    features.extend(_levels(domains))\n",
    "    features.append(_tlds(domains))\n",
    "    features.append(_distinctchars(domains))\n",
    "    return features\n",
    "\n",
    "def _build_ground_truth_dataframe(alpha=10, dgadir='../data/dga/ground-truth'):\n",
    "    X, y = [], []\n",
    "    for class_, domains in tqdm(_load_ground_truth(dgadir=dgadir).items(), desc='Classes', position=0):\n",
    "        if class_ in set(['murofet', 'suppobox', 'symmi', 'sisron', 'ranbyus', 'corebot', 'padcrypt', 'dircrypt', 'gozi', 'locky', 'pykspa', 'dnschanger']):\n",
    "            continue\n",
    "        for domain_chunks in chunks(domains, alpha):\n",
    "            features = vectorize(domain_chunks)\n",
    "            if len(features) != 134:\n",
    "                sys.stderr.write('Error with chunk from %s\\n' % class_)\n",
    "                continue\n",
    "            X.append(features)\n",
    "            y.append(class_)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def _heatmap(crosstab):\n",
    "    plt.clf()\n",
    "    p = seaborn.heatmap(crosstab, square=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def _cv(X, y, k, name, clf, csvname, modeldir=None, terms=None, resultdir=None):\n",
    "    print('## %s' % name)\n",
    "    print('### Cross Validation')\n",
    "    print('`%s`' % str(cross_val_score(clf, X, y, cv=k)))\n",
    "    print('### CV Confusion Matrix')\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=k)\n",
    "    print('```')\n",
    "    print(pd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted']))\n",
    "    print('```')\n",
    "\n",
    "    _heatmap(pd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted'],\n",
    "                         normalize='index'))\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def train():\n",
    "    pass\n",
    "\n",
    "# * Generate feature vector from set of domains\n",
    "# ** Important to note that it needs to chunk em up for the classifier to work. Refer to Pleiades paper.\n",
    "# ** Chunks into groups of alpha = 10\n",
    "# ** 1,2,3,4-gram. get mean, median, std. dev. (4 x 3 = 12 features)\n",
    "# ** shannon entropy of 2ld/3ld. exact value per domain (2x), mean/median of group (2 x 2) total of 6 features.\n",
    "# ** mean, med, std. dev, var of length, # of domain levels, distinct chars, # distinct TLDs, # .com, # .other, ratio of #.com/#.other, mean/med/stddev TLDs\n",
    "# * Train model\n",
    "# * Fit cluster\n",
    "# * Output as JSON maybe huh\n",
    "\n",
    "def save(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
